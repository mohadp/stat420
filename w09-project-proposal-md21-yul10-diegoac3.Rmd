---
title: "Data Analysis - Project Proposal"
author: "Mohamed Diakite Pineda, Yu Liu, Diego Carreno, STAT 420, Summer 2020"
date: "7/19/2020"
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

## Student names

- Mohamed Diakite Pineda, *md21@*
- Yu Liu, *yul10@*
- Diego Carreno, *diegoac3@*

## Project Title: Start-up acquisition price

We intent to find out what makes a start-up attractive for acquisition, and what drives its acquisition price up or down.

## Description of the data file

In this study, we plan to analyze the acquisition data of more than 2000 companies from all over the world, which are founded from year 1900 to year 2014. Particulary, we have interests in exploring what factors impact the acquisition price of a company, and trying to predict the company's acquisition price using regression. The data set has 2473 rows of data.

We consider the following variable as the response variable:

| variable name | description | data scoure | data type | variable type | # of missing records |
|:-:|:-:|:-:|:-:|:-:|:-:|
| aquisition_price_amount | Amount paid for aquisition | acquisitions.csv | int | numerical | 0 |

We consider the following 14 variables as the potential predictor:

| variable name | description | data scoure | data type | variable type | # of missing records |
|:-:|:-:|:-:|:-:|:-:|:-:|
| category_code | Entity category | objects.csv | string | categorical | 475 |
| normalized_name | Normalized entity name | objects.csv | string | categorical | 0 |
| logo_width | Logo width | objects.csv | float | numerical | 0 |
| logo_height | Logo height | objects.csv | float | numerical | 0 |
| description | Description of the entity | objects.csv | string | categorical | 1380 |
| country_code | Country code | objects.csv | string | categorical | 514 |
| state_code | State code | objects.csv | string | categorical | 980 |
| city | City name | objects.csv | string | categorical | 580 |
| region | Region name | objects.csv | string | categorical | 1110 |
| investment_rounds | Number of investment round participated in | objects.csv | int | numerical | 0 |
| investment_companies | Number of companies invested in | objects.csv | int | numerical | 0 |
| milestones | Number of milestones the entity has | objects.csv | int | numerical | 0 |
| relationships | Number of relationships the entity has | objects.csv | int | numerical | 0 |
| acquired_at | Date of deal | acquisition.csv | timestamp | numerical | 0 |


## Background information of the data set

The database comes from [a Kaggle public dataset about startup investments](https://www.kaggle.com/justinas/startup-investments?select=funding_rounds.csv%29) published by Justinas Cirtautas at the end of 2019. The dataset covers more 450 thousand company information from all over the world till 2013. Among those, about 10 thousand companies acomplished their acquisition successfully, while 80% of them got 0 dollars in their acquisition. 

The entire dataset provides 11 cvs file with 154 columns, which covers six aspects of the startup ecosystem including organizations, individuals, company news, funding rounds, acquisitions, and IPOs. We populated all the 11 csv files. More detials can be found in the appendix at the end of this proposal. Considering the missing data rate, we determine to focus on only two files: acquisition.csv and objects.csv.

- acquisition.csv provides the acquisition details, including the acquisition amount, currency and the deal date.
- objects.csv provides the basic company profiles, which can be used as the predictor of our study.


## Purpose statement
Venture capital investments hits 9.5 billion U.S. dollars in the internet industry in the United States, as of first quarter 2020. Other leading VC sectors in terms of investment were healthcare, and software. Inspired by the florish startup world, we want to investigate what factors can impact the company price during acquisition.

In this study, we select the price of the company for acquisition as our 'Y', the response variable. Our target is to find a set of predictors and use regression method to predict our Y. We checked all dataset and select 14 varaibles as our potential predictors. Later on we will do feature engineering to determine our X's in the regression.

Meanwhile, we also want to know some interesting questions, like, whether the company name and the logo size has any impact on the acquisition price, or how the acquisition distributed by geography. We believe the data will tell us the answer after finishing this project. 

## Loading the data into `R`
We merged two csv file to get our final database `startups.csv`. For the details in data cleaning, please refer to the Appendix at the end of the proposal.

Below is the snapshot to show we load our data into `R` successfully. We are including a subset of the columns for simplicity, because some columns have very long text entries.

```{r}
sample_data = read.csv("startups.csv")

knitr::kable(head(sample_data[, c(
  "aquisition_price_amount",
  "city",
  "funding_total_usd",
  "category_code",
  "funding_rounds",
  "milestones",
  "relationships"
)]))
```


## Appendix: Data Pre-cleaning

---
title: "Startup Data"
output: html_document, startups.csv
---

### Data load 

The startup data comes from Kaggle: [https://www.kaggle.com/justinas/startup-investments]

This dataset contains information about the startup ecosystem: organizations, individuals, company news, funding rounds, acquisitions, and IPOs, extracted originally from the Crunchabse Data website. In this project, only 6 of the 11 tables will be used and joined using unique IDs. 

- **People Data Loading:**

```{r message=FALSE}

library(readr)

#People
people = read_csv("./people.csv")
colnames(people)[2] = "person_id"
attr(people, "spec") = NULL
colnames(people)

```


- **People Education Data Loading:**


```{r message=FALSE}

degrees = read_csv("./degrees.csv")
colnames(degrees)[2] = "person_id"
attr(degrees, "spec") = NULL
colnames(degrees)

```


- **Companies Data Loading:**

```{r message=FALSE, warning=FALSE}
#Company
companies = read_csv("./objects.csv")
attr(companies, "spec") = NULL
attr(companies, "problems") = NULL
colnames(companies)[1] = "company_id"
colnames(companies)
```


- **People/Company Relationship Data Loading:**

```{r message=FALSE}
comp_peep = read_csv("./relationships.csv")
colnames(comp_peep)[3] = "person_id"
colnames(comp_peep)[4] = "company_id"
colnames(comp_peep)
attr(comp_peep, "spec") = NULL
#str(comp_peep)
```


- **Company Funding Data Loading:**

```{r message=FALSE}
#install.packages("reshape")
library(reshape)
library(data.table)

invest_rounds = read_csv("./funding_rounds.csv")
attr(invest_rounds, "spec") = NULL
colnames(invest_rounds)[3]="company_id"
colnames(invest_rounds)

```


- **Company Aquisition Data Loading:**


```{r message=FALSE}
aquisitions = read_csv("./acquisitions.csv")
colnames(aquisitions)[4] = "company_id"
colnames(aquisitions)[6] = "aquisition_price_amount"

attr(aquisitions, "spec") = NULL

```


### Data Preparation

After loading the individual tables, we need to join all into a single dataset that can be used for analysis. 

#### Company Founders

The people, degrees and their relationship to the companies can all be joined into a sigle people dimension table. As part of the process, we will "normalize" the data, particularly the discreet data for people so it can be used for regression.

```{r}
#Join people with their degrees information and with which company these people belong to.
full_peep = merge(people, degrees, by="person_id")
founders = merge(full_peep, comp_peep, by="person_id")


#Identify founders and CEOs only

#get rows with founder, ceos, chairmans only
leads = c(grep("founder", ignore.case = TRUE, x = founders$title), grep("ceo", ignore.case = TRUE, x = founders$title), grep("chairman", ignore.case = TRUE, x = founders$title))
founders[grep("founder", ignore.case = TRUE, x = founders$title), "title"] = "Founder"
founder_leads = founders[leads, ]

#Auxiliary functions to count rows for analysis of data
count_disticts = function(x){
  length(unique(x))
}
counts = function(x){
  length(x)
}
```


"Normalize" the title information for the founders' degree type so that it can be used as a potential regression predictor.

```{r}
founder_leads_clean = founder_leads
founder_leads_clean[grep("MS ", ignore.case = TRUE, x = founder_leads_clean$degree_type), c("degree_type")] = "MS"
founder_leads_clean[grep(
  "Master in Business|Master of Business|M\\.B\\.A\\.|MBA|Executive",
  ignore.case = TRUE,
  x = founder_leads_clean$degree_type
), c("degree_type")] = "MBA"
founder_leads_clean[grep("Ph|doc|pos.*grad",
                         ignore.case = TRUE,
                         x = founder_leads_clean$degree_type), c("degree_type")] = "Ph.D."
founder_leads_clean[grep(
  "Bachelor of Science|BSc|B\\.S|B\\.E|BE|BS|Ing|Eng|Bsc|SCB",
  ignore.case = TRUE,
  x = founder_leads_clean$degree_type
), c("degree_type")] = "BS"
founder_leads_clean[grep("Bachelor|BA|B\\.A",
                         ignore.case = TRUE,
                         x = founder_leads_clean$degree_type), c("degree_type")] = "BA"
founder_leads_clean[grep("B[A-Z][A-Z]+|B\\.|BC",
                         ignore.case = FALSE,
                         x = founder_leads_clean$degree_type), c("degree_type")] = "Bachelors"
founder_leads_clean[grep("MS|M\\.S", ignore.case = TRUE, x = founder_leads_clean$degree_type), c("degree_type")] = "MS"
founder_leads_clean[grep("Master|Graduate|Fellow",
                         ignore.case = TRUE,
                         x = founder_leads_clean$degree_type), c("degree_type")] = "Masters"
founder_leads_clean[grep("M[A-Z]+|M.",
                         ignore.case = FALSE,
                         x = founder_leads_clean$degree_type), c("degree_type")] = "Masters"
founder_leads_clean[grep(
  "No.*degre|high.*school|secondary educa|no.*complete|course|incomplete|none|did not|prepa|[0-9].*[0-9]+ Grade",
  ignore.case = TRUE,
  x = founder_leads_clean$degree_type
), c("degree_type")] = "No Degree"
founder_leads_clean[grep("Law|JD|J\\.D\\.",
                         ignore.case = FALSE,
                         x = founder_leads_clean$degree_type), c("degree_type")] = "Law"
founder_leads_clean[grep("Business", ignore.case = TRUE, x = founder_leads_clean$degree_type), c("degree_type")] = "MBA"
founder_leads_clean[grep(
  "Undergr|Diplom|degree|college|under grad|lic|dipl",
  ignore.case = TRUE,
  x = founder_leads_clean$degree_type
), c("degree_type")] = "Bachelors"
founder_leads_clean[!(
  founder_leads_clean$degree_type %in% c(
    "MS",
    "MBA",
    "Ph.D.",
    "BS",
    "BA",
    "Bachelors",
    "MS",
    "Masters",
    "No Degree",
    "Law",
    "MBA"
  )
), c("degree_type")] = "Degree"



## Remove nulls and anly selects columns relevant for analysis.
founder_leads_clean = na.omit(founder_leads_clean[, c("degree_type", "subject", "institution", "company_id", "title")])
founder_leads_clean = founder_leads_clean[order(founder_leads_clean$company_id), ]


```

The code below reshapes the people subdataset into a wider table that has founder information per company, each in a single row of data. In other workds, the code below transposes the founder information (education background) to the columns.

```{r}

founders_bg = cbind(
  leader1_degree_type = rep(0, nrow(founder_leads_clean)), leader1_subject = rep(0, nrow(founder_leads_clean)), leader1_institution = rep(0, nrow(founder_leads_clean)), 
  leader2_degree_type = rep(0, nrow(founder_leads_clean)), leader2_subject = rep(0, nrow(founder_leads_clean)), leader2_institution = rep(0, nrow(founder_leads_clean)), 
  founders_ceos_chairmans = rep(0, nrow(founder_leads_clean)))

nleads = 2
prev_company = "c"
company_index = 0
l = 0
for(i in  1:nrow(founder_leads_clean)){
  company = founder_leads_clean$company_id[i]
  if(prev_company == company){
    l = l + 1
  }else{
    prev_company = company
    company_index = i
    l = 0
  }
  if(l < nleads){
    founders_bg[company_index,l*3+1] = founder_leads_clean$degree_type[i]
    founders_bg[company_index,l*3+2] = founder_leads_clean$subject[i]
    founders_bg[company_index,l*3+3] = founder_leads_clean$institution[i]
    #if(l == 0) founders_bg[company_index,7] = length(founder_leads_clean$company_id[founder_leads_clean$company_id == company & founder_leads_clean$title == "Founder"])
  }
}

```

We also remove some obvious rows with no data. 

```{r}
founders_clean = cbind(founder_leads_clean, as.data.frame(founders_bg))
founders_clean = founders_clean[founders_clean$leader1_degree_type != "0",-5]
founders_clean = na.omit(founders_clean)

```


#### Investment & Aquisition Data 

Investment data is stored in "long" shape, with every company having multiple rounds of investment (Series A-F, debt, angel, seed, etc.), each stored in a single row. Below is the process of reshaping the inventment rounds to a wide shape, in which every row represents all the investment rounds for a single company.


```{r}

invest_rounds = invest_rounds[, c(
  "company_id",
  "funding_round_code",
  "raised_amount_usd",
  "pre_money_valuation_usd",
  "post_money_valuation_usd",
  "participants"
)]

rounds_table = as.data.table(invest_rounds)
rounds_table = melt(
  data = rounds_table,
  id.vars = c("company_id", "funding_round_code"),
  measure.vars = c(
    "raised_amount_usd",
    "pre_money_valuation_usd",
    "post_money_valuation_usd",
    "participants"
  ),
  variable_name = "value"
)
invest_rounds = na.omit(as.data.frame(
  dcast(
    data = rounds_table,
    formula = company_id ~ funding_round_code + variable,
    fun.aggregate = sum
  )
))

```

Filter out companies with aquisition amount not in USD (all other variable/predictors are in USD)
```{r, include=FALSE}
aquisitions = aquisitions[aquisitions$price_currency_code == "USD",c("company_id", "aquisition_price_amount")]

```


### Final Dataset For Analysis

After extracting the individual data aspects (people, company, investments) of the companies, these aspects are joined into a single dataset. The dataset that will be used for this project will depend on the resulting row numbers after different combination of joins.

```{r}
company_aq = company_data = merge(companies, aquisitions, by = "company_id")
nrow(company_aq)
company_inv = merge(company_data, invest_rounds, by = "company_id")
nrow(company_inv)
company_aq_lds = merge(company_data, founders_clean, by = "company_id")
nrow(company_aq_lds)
```

### Preminiary Analysis

To ensure that the dataset is viable for linear regression analysis, we create a preliminary linear model with company aquisition price as the response variable, and several other potential columns in the startups dataset as predictors. We are using the `company_aq` dataset above.

```{r}
# Data set to be analized, filtering out aquisition values without aquisition price
startups = company_aq[company_aq$aquisition_price_amount>0, ]
#write_csv(x = startups, path = "./startups.csv")
```

The dataset to be used, `startups` has `r nrow(startups)` rows of data.

A sample output of the data is below:

```{r echo=FALSE}
#Sample out
knitr::kable(head(startups[, c(
  "aquisition_price_amount",
  "city",
  "funding_total_usd",
  "category_code",
  "funding_rounds",
  "milestones",
  "relationships"
)]))
```

Moreover, a very simple model illustrates feasibility for linear regression analysis:

```{r}
#Linear Model
company_model = lm(log(aquisition_price_amount) ~ city+ log(funding_total_usd+1) + category_code + funding_rounds + milestones + relationships, data=startups)
```

- $R^2= `r summary(company_model)$r.squared`$
- $\text{p-value} = `r pf(summary(company_model)$fstatistic[1], summary(company_model)$fstatistic[2], summary(company_model)$fstatistic[3], lower.tail=FALSE)`$

In addition, we can see how the linear regression assumptions of normality and constant variance are with the simple model. In the study, will better adjust tthe model in question to more appropriately describe the data with the model including its variance.

```{r message=FALSE, warning=FALSE}
#Plots of the model
par(mfrow = c(1,2))
plot(company_model, which = 1:2)
```


