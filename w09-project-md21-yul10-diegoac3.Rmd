---
title: "Data Analysis - Project Proposal"
author: "Mohamed Diakite Pineda, Yu Liu, Diego Carreno, STAT 420, Summer 2020"
date: "8/06/2020"
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: inline
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

## Names

- Mohamed Diakite Pineda, *md21@*
- Yu Liu, *yul10@*
- Diego Carreno, *diegoac3@*


## Introduction

We intent to find out what makes a start-up attractive for acquisition, and what drives its acquisition price up or down.

In this study, we plan to analyze the acquisition data of more than 2000 companies from all over the world, which are founded from year 1900 to year 2014. Particulary, we have interests in exploring what factors impact the acquisition price of a company, and trying to predict the company's acquisition price using regression. The data set has 2473 rows of data.

We consider the following variable as the response variable:

| variable name | description | data scoure | data type | variable type | # of missing records |
|:-:|:-:|:-:|:-:|:-:|:-:|
| aquisition_price_amount | Amount paid for aquisition | acquisitions.csv | int | numerical | 0 |

We consider the following 14 variables as the potential predictor:

| variable name | description | data scoure | data type | variable type | # of missing records |
|:-:|:-:|:-:|:-:|:-:|:-:|
| category_code | Entity category | objects.csv | string | categorical | 475 |
| normalized_name | Normalized entity name | objects.csv | string | categorical | 0 |
| logo_width | Logo width | objects.csv | float | numerical | 0 |
| logo_height | Logo height | objects.csv | float | numerical | 0 |
| description | Description of the entity | objects.csv | string | categorical | 1380 |
| country_code | Country code | objects.csv | string | categorical | 514 |
| state_code | State code | objects.csv | string | categorical | 980 |
| city | City name | objects.csv | string | categorical | 580 |
| region | Region name | objects.csv | string | categorical | 1110 |
| investment_rounds | Number of investment round participated in | objects.csv | int | numerical | 0 |
| investment_companies | Number of companies invested in | objects.csv | int | numerical | 0 |
| milestones | Number of milestones the entity has | objects.csv | int | numerical | 0 |
| relationships | Number of relationships the entity has | objects.csv | int | numerical | 0 |
| acquired_at | Date of deal | acquisition.csv | timestamp | numerical | 0 |


### Background information of the data set

The database comes from [a Kaggle public dataset about startup investments](https://www.kaggle.com/justinas/startup-investments?select=funding_rounds.csv%29) published by Justinas Cirtautas at the end of 2019. The dataset covers more 450 thousand company information from all over the world till 2013. Among those, about 10 thousand companies acomplished their acquisition successfully, while 80% of them got 0 dollars in their acquisition. 

The entire dataset provides 11 cvs file with 154 columns, which covers six aspects of the startup ecosystem including organizations, individuals, company news, funding rounds, acquisitions, and IPOs. We populated all the 11 csv files. More detials can be found in the appendix at the end of this proposal. Considering the missing data rate, we determine to focus on only two files: acquisition.csv and objects.csv.

- acquisition.csv provides the acquisition details, including the acquisition amount, currency and the deal date.
- objects.csv provides the basic company profiles, which can be used as the predictor of our study.


### Purpose statement
Venture capital investments hits 9.5 billion U.S. dollars in the internet industry in the United States, as of first quarter 2020. Other leading VC sectors in terms of investment were healthcare, and software. Inspired by the florish startup world, we want to investigate what factors can impact the company price during acquisition.

In this study, we select the price of the company for acquisition as our 'Y', the response variable. Our target is to find a set of predictors and use regression method to predict our Y. We checked all dataset and select 14 varaibles as our potential predictors. Later on we will do feature engineering to determine our X's in the regression.

Meanwhile, we also want to know some interesting questions, like, whether the company name and the logo size has any impact on the acquisition price, or how the acquisition distributed by geography.

### Loading the data into `R`
We merged two csv file to get our final database `startups.csv`. For the details in data cleaning, please refer to the Appendix at the end of the proposal.

Below is the snapshot to show we load our data into `R` successfully. We are including a subset of the columns for simplicity, because some columns have very long text entries.

```{r}
sample_data = read.csv("startups.csv")


knitr::kable(head(sample_data[, c(
  "aquisition_price_amount",
  "city",
  "funding_total_usd",
  "category_code",
  "funding_rounds",
  "milestones",
  "relationships"
)]))

str(sample_data)
```


## Methods




### Data Exploration (Yu)


### Data Adjustments (Moha)

Based on the single predictor analysis above, it is now possible to further simplify the dataset that will be used to determine whether the `aquisition_price` of startups can be correlated with startup characteristics and their performance. 

Some data adjustments include:

- Collapsing of the number of levels for factor variables a high number levels: `category_code`, `country_code`, `city`, `state_code`. This will ommit potential dummy variables when dealing with factor variables with levels that provide little significance. This also adds benefit of improving model simplicity by reducing number of levels.
- Removal of columns that are necessary or useful in the statitical analysis: `company_id`,`normalized_name`, `status`, `logo_width`, `logo_height`.
- Division of the full data into 2 datasets, one for training (`startups_tr`), with  80% of the observations; the rest will be included in a test dataset (`startups_te`), useful for model cross-validation. The split is done randomly.
- Removing outlier case of acquisition for 2.6T. For reference, Amazon's Market Cap is 1.6T. We believe this was an input error.


```{r message=FALSE}

#Loading of the data
library(readr)
startups = read_csv("./startups.csv")
attr(startups, "spec") = NULL
n = nrow(startups)

#Function that reduces the number of levels of a factor, by replacing some levels with the new.level based on the number of records a level has. 
#If threshold < 1, then it keeps the largest levels by counts up to when a the threshold percentile is reached
#If threshold >= 1, then it keeps the levels that have counts greater or equal to the threshold.
shorten_levels = function(col, threshold, new.level){
  new_col = as.character(col) 
  levels_to_collapse = 0
  level_counts = summary(col)
  counts = aggregate(new_col, by = list(col = new_col), FUN = length)

  if(threshold < 1){
    counts = counts[order(counts$x, decreasing = TRUE), ]
    counts$cumpct = cumsum(counts$x)/sum(counts$x)
    levels_to_collapse = counts[counts$cumpct >= threshold, "col"]
  }else{
    #levels_to_collapse = names(which(level_counts > threshold))
    levels_to_collapse = counts[counts$x < threshold, "col"]
  }
  
  replace = new_col %in% levels_to_collapse
  new_col[replace] = new.level
  as.factor(new_col)
}

#Modified function from https://www.mjdenny.com/Text_Processing_In_R.html to "normalize text"
clean_string = function(string){
    # Lowercase
    temp =  tolower(string)
    # Remove everything that is not a number or letter (
    temp = stringr::str_replace_all(temp,"[^a-zA-Z\\s]", " ")
    # Shrink down to just one white space
    temp = stringr::str_replace_all(temp,"[\\s]+", " ")
    temp
}

#Collapse the number of levels for all factor predictors.
startups$category_code = shorten_levels(startups$category_code, .90, "other")
startups$country_code = shorten_levels(startups$country_code, 20, "other")
startups$city = shorten_levels(clean_string(as.character(startups$city)),20, "other")
startups$state_code = shorten_levels(as.character(startups$state_code),.90, "other")
summary(startups$state_code)

#Remove unnecessary columns (particularly the descriptions and single-level factors and variables that did not seem relevant for the aquisition response variable)
to_remove = c("company_id","normalized_name", "status", "logo_width", "logo_height")
startups_desc = startups[, to_remove]
startups = startups[, -which(colnames(startups) %in% to_remove)]

#Remove outlier in acquisition price
startups = startups[startups$aquisition_price_amount<1e12,]

#Split of data for cross-validation: 80% for training, 20$ for testing.
set.seed(420)
train_index = sample(1:nrow(startups), size = floor(0.8 * nrow(startups)))
startups_tr = startups[train_index,]
startups_te = startups[-train_index,]

```

The final dataset for analysis includes the following predictors. 

```{r}
str(startups)
```


### Diagnostics and Performance Function (Diego)

In order to compare models, we use a single function that runs tests on both:

- Model assumptions: Residual vs fitted and Q-Q plots, bptest and wilk-shapiro test.
- Model performance: Adjusted r-squared, AIC, BIC, leave one out cross-validation, regression p-value, and rmse on test data.

```{r message=FALSE}

library(lmtest)
library(MASS)
library(faraway)

diagnostics_performance = function(model,
                       predict_function = function (x) {x},
                       pcol = "grey",
                       lcol = "dodgerblue",
                       plotit = TRUE,
                       testit = TRUE) {
  #Plot functionality
  if (plotit) {
    par(mfrow = c(1, 2))
    
    #Fitted vs residuals plot
    plot(
      resid(model) ~ fitted(model),
      col = pcol,
      pch = 20,
      main = "Fitted vs Residuals Plot",
      xlab = "Fitted",
      ylab = "Residuals"
    )
    abline(h = 0, col = lcol, lwd = 2)
    
    #Q-Q plot
    qqnorm(resid(model), col = pcol, main = "Normal Q-Q Plot")
    qqline(resid(model),
           lty = 2,
           lwd = 2,
           col = lcol)
  }
  
  #Test functionality
  if (testit) {
    #P-val from Shapiro-Wilk test to assess normality
    p_val_shapiro = shapiro.test(resid(model))$p.value
    
    #P-val from BP test to assess cosntant variance
    p_val_bp = unname(bptest(model)$p.value)
    
    #RMSE
    RMSE = sqrt(mean(sum(predict_function(predict(model, newdata = startups_te)) - startups_te$aquisition_price_amount)^2))
    
    #LOOCV
    RMSE_LOOCV = sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
    
    #R squqred
    r_squared = summary(model)$r.squared
    
    #Adjusted r squared
    adj_r_squared = summary(model)$adj.r.squared
    
    #AIC
    AIC = extractAIC(model)[2]
    
    #BIC
    BIC =  extractAIC(model, k = log(n))[2]
    
    #Regression p-value
    reg_p_value = unname(pf(summary(model)$fstatistic[1],summary(model)$fstatistic[2],summary(model)$fstatistic[3], lower.tail=FALSE))
    
    #Output from test
    list(p_val_shapiro = p_val_shapiro,
         p_val_bp = p_val_bp,
         RMSE = RMSE,
         RMSE_LOOCV = RMSE_LOOCV,
         r_squared = r_squared,
         adj_r_squared = adj_r_squared,
         reg_p_value = reg_p_value,
         AIC = AIC,
         BIC = BIC
         )
  }
}

```


### Model building

- **ADDITIVE MODELS**

*All variables*

The first step of the exploration is to look at a simple additive model that contains all variables.

```{r}
#Training the model
additive_full = lm(aquisition_price_amount ~ . , data = startups_tr)

#Diagnostics and performance of the model
diagnostics_performance(additive_full)

#Variance inflation factors
faraway::vif(additive_full)[faraway::vif(additive_full)>5]

#Outliers
c(
  num_outliers = length(rstandard(additive_full)[abs(rstandard(additive_full)) > 2]),
  pct_outliers = length(rstandard(additive_full)[abs(rstandard(additive_full)) > 2]) / length(rstandard(additive_full))
)

#Influential points
c(
  num_influential_points = sum(cooks.distance(additive_full) > 4 / length(cooks.distance(additive_full))),
  pct_influential_points = sum(cooks.distance(additive_full) > 4 / length(cooks.distance(additive_full))) / length(cooks.distance(additive_full))
)
```

We can see that model assumptions are clearly violated. We also see that there are variables with a potentially large variance inflation factor. For example, it would make sense to only use investment_rounds or invested_companies, or country or state, because there can be collinearity as discussed in the data exploration section. 1.97% and 2.73% of points are considered to be outliers and to have influence respectevely. Although we can aim to reduce these, in this first exploration of different additive models it does not seem likely this values will change much.

```{r}
#Direct correlation between investment_rounds and invested_companies is close to one
cor(startups_tr$investment_rounds,startups_tr$invested_companies)

#The partial correlation coefficient is also very close to zero.
#The variation of invested_companies that is unexplained by remaining predictors shows little correlation
#with the variation of aquisition_price_amount that is not explained remaining predictors.
#Adding invested companies is of little use to the model.
model_test_invested_companies_1 = lm(invested_companies ~ .-aquisition_price_amount , data = startups_tr)
model_test_invested_companies_2 = lm(aquisition_price_amount ~ .-invested_companies , data = startups_tr)
cor(resid(model_test_invested_companies_1), resid(model_test_invested_companies_2))


```

We remove the invested_companies and state_code from the predictors and train a new model.

*Improving collinearity*

```{r}
#Training the model
additive_full_updated = lm(aquisition_price_amount ~ .-investment_rounds -state_code , data = startups_tr)

#Diagnostics and performance of the model
diagnostics_performance(additive_full_updated)

#Variance inflation factors
faraway::vif(additive_full_updated)[faraway::vif(additive_full_updated)>5]

#Outliers
c(
  num_outliers = length(rstandard(additive_full_updated)[abs(rstandard(additive_full_updated)) > 2]),
  pct_outliers = length(rstandard(additive_full_updated)[abs(rstandard(additive_full_updated)) > 2]) / length(rstandard(additive_full_updated))
)

#Influential points
c(
  num_influential_points = sum(
    cooks.distance(additive_full_updated) > 4 / length(cooks.distance(additive_full_updated))
  ),
  pct_influential_points = sum(
    cooks.distance(additive_full_updated) > 4 / length(cooks.distance(additive_full_updated))
  ) / length(cooks.distance(additive_full_updated))
)

```

In the plots, we can see that normality and constant variance are still suspect. We also see confirm that normality suspect due to the low shapiro-wilk test p-value, and that the constant variance is suspect due to low bp test p-value. The R squared and adjusted R squared remain somewhat similar, as well as the RMSE leave one out cross validation. But we now have less variables with a potentially large collinearity. Outlier and influential points remain somewhat similar.

*Backwards AIC*

Now we will perform a backwards AIC search to optimize for AIC and further decrease collinearity issues.

```{r}
#Finding the model
model_additive_selected = step(additive_full_updated, trace = 0)

#Diagnostics and performance of the model
diagnostics_performance(model_additive_selected)

#Variance inflation factors
faraway::vif(model_additive_selected)[faraway::vif(model_additive_selected)>5]

#Outliers
c(
  num_outliers = length(rstandard(model_additive_selected)[abs(rstandard(model_additive_selected)) > 2]),
  pct_outliers = length(rstandard(model_additive_selected)[abs(rstandard(model_additive_selected)) > 2]) / length(rstandard(model_additive_selected))
)

#Influential points
c(
  num_influential_points = sum(cooks.distance(model_additive_selected) > 4 / length(cooks.distance(model_additive_selected))),
  pct_influential_points = sum(cooks.distance(model_additive_selected) > 4 / length(cooks.distance(model_additive_selected))) / length(cooks.distance(model_additive_selected))
)


```

The found model still violates normality and constant variance assumptions, as would normally be expected since this is derived from a model that also violated them. However, we are able to see less predictors with a variance inflation factor higher than 5. Outlier and influential points remain similar. Now we proceed to compare both models with an anova test and decide which is the best additive model.

*Selecting the best additive model*

```{r}
anova(model_additive_selected, additive_full_updated)
```

We can see that the larger model doesn't offer a statistically significant difference in explaining the relationship of predictors and response. For that reason, we prefer the model found through the backwards BIC method, `model_additive_selected`. This model still violates normality and constant variance assumptions, and these will not likely be corrected or at least further improved until we look at more complex interaction models or we apply transformations to predictor or response variables (done below).


- **INTERACTION MODELS**

*Two-way interactions*

We started the exploration by training a 2-way interaction model, excluding investment_rounds and state_code from the variables, due to collinearity.

We also tried to fit an overall 3-way model in a couple of ways. First, by considering all terms besides investment_rounds and  state_code, but due to computational limitations we decided to take a different approach (after 3 hours the model was not complete training yet). We also tried to fit an overall 3-way model, excluding city from the interaction terms (and adding it as a separate term on its own) to try to improve machine performance--altough this model offered a good initial R squared (~0.9) it was hard to interpret and still taxing on machine resources.

```{r}
#Training the model
interaction_model = lm(aquisition_price_amount ~ (. -investment_rounds -state_code)^2 , data = startups_tr)

#Diagnostics and performance of the model
diagnostics_performance(interaction_model_simple)

#Number of predictosr with high inflation factors
length(faraway::vif(interaction_model)[faraway::vif(interaction_model)>5])

```




- **TRANSFORMATION MODELS**



### Moha

```{r}
str(startups_tr)
library(lmtest)

plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

diagnostics2 = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.50, plotit = TRUE, testit = TRUE){
  
  if(plotit){
    par(mfrow=c(1,2))
    plot(fitted(model), resid(model), col =  pcol, pch = 20, xlab = "Fitted", ylab = "Residuals", main = paste("Data from", deparse(substitute(model))))
    abline(h = 0, col = lcol, lwd = 2)
    
    qqnorm(resid(model), main = paste("Normal Q-Q Plot from",deparse(substitute(model))), col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  
  if(testit){
    shatest = shapiro.test(resid(model))
    bp = bptest(model)
    p = length(coef(model))
    n = length(resid(model))
    rsme_looocv = sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
    
    
    
    res = t(c(
      "r2" =  summary(model)$r.squared,
      "shapiro" =  shatest$p.value,
      "bptest" =  bp$p.value,
      "p" =  p,
      "AIC" =  extractAIC(model)[2],
      "BIC" =  extractAIC(model, k = log(n))[2],
      "adjR2" =  summary(model)$adj.r.squared,
      "rsme_looocv" =  rsme_looocv))
    res
  }
}
```

```{r}
#str(startups_tr)
additive_all_model = lm(log(aquisition_price_amount) ~ . - invested_companies -top25pct -top50pct , data = startups_te)
summary(additive_all_model)
#diagnostics2(additive_all_model, alpha = .01)

numeric_cols = which(sapply(startups_tr, typeof)  == "double")
numeric_cols



cor(startups_tr[,numeric_cols])


```

```{r}
vifs = faraway::vif(additive_all_model)
vifs[vifs > 5]
```

```{r}
plot(x = log(startups_tr$name_length), y = startups_tr$aquisition_price_amount, col = startups_tr$istop25+1)
plot(x = log(startups_tr$name_length), y = startups_tr$aquisition_price_amount, col = startups_tr$istop25+1)
plot(x = log(startups_tr$name_length), y = startups_tr$aquisition_price_amount, col = startups_tr$istop25+1)
plot(x = log(startups_tr$name_length), y = startups_tr$aquisition_price_amount, col = startups_tr$istop25+1)
```



After the selection of columns described in the **Appendix 1**, we need to identify which of the predictors are relevant to a potential linear model that describes the aquisition value.

```{r}
#add_all_model = lm()
```




## Results

### Diego


### Below


## Discussion







## Appendix: Data Pre-cleaning



### Data load 

The startup data comes from Kaggle: [https://www.kaggle.com/justinas/startup-investments]

This dataset contains information about the startup ecosystem: organizations, individuals, company news, funding rounds, acquisitions, and IPOs, extracted originally from the Crunchabse Data website. In this project, only 6 of the 11 tables will be used and joined using unique IDs. 

- **People Data Loading:**

```{r message=FALSE, eval=FALSE}

library(readr)

#People
people = read_csv("./people.csv")
colnames(people)[2] = "person_id"
attr(people, "spec") = NULL
colnames(people)

```


- **People Education Data Loading:**


```{r message=FALSE, eval=FALSE}

degrees = read_csv("./degrees.csv")
colnames(degrees)[2] = "person_id"
attr(degrees, "spec") = NULL
colnames(degrees)

```


- **Companies Data Loading:**

```{r message=FALSE, warning=FALSE, eval=FALSE}
#Company
companies = read_csv("./objects.csv")
attr(companies, "spec") = NULL
attr(companies, "problems") = NULL
colnames(companies)[1] = "company_id"
colnames(companies)
```


- **People/Company Relationship Data Loading:**

```{r message=FALSE, eval=FALSE}
comp_peep = read_csv("./relationships.csv")
colnames(comp_peep)[3] = "person_id"
colnames(comp_peep)[4] = "company_id"
colnames(comp_peep)
attr(comp_peep, "spec") = NULL
#str(comp_peep)
```



- **Company Aquisition Data Loading:**


```{r message=FALSE, eval=FALSE}
aquisitions = read_csv("./acquisitions.csv")
colnames(aquisitions)[4] = "company_id"
colnames(aquisitions)[6] = "aquisition_price_amount"
attr(aquisitions, "spec") = NULL
colnames(aquisitions)

#Filter out companies with aquisition amount not in USD (all other variable/predictors are in USD)
aquisitions = aquisitions[aquisitions$price_currency_code == "USD",c("company_id", "aquisition_price_amount")]
```



### Data Preparation

After loading the individual tables, we need to join all into a single dataset that can be used for analysis. 

#### Company Founders

The people, degrees and their relationship to the companies can all be joined into a sigle people dimension table. Additionally, we can extract potential relevant predictors for regression. For every person, generally an executive, we identify in each company people that attended to any of the top 25 or top 50 universities in the world ("top 50" excludes the top 25 universities). Also, we count the number of people in each company as well as the count of people by company that attended the top 25 and top 50 universities. Finally, to normalize these numbers, we also additional columns that represent percentage of people in each company that attended the top 25 and top 50 universities.

```{r eval=FALSE}
#Auxiliary functions to count rows for analysis of data
count_disticts = function(x){
  length(unique(x))
}
counts = function(x){
  length(x)
}

#Join people with their degrees information and with which company these people belong to.
full_peep = merge(people, degrees, by="person_id")
founders = merge(full_peep, comp_peep, by="person_id")

top1 = grep(".*University of Oxford.*|.*xford.*|.*California Institute of Technology.*|.*altech|.*University of Cambridge.*|.*ambridge.*|.*Stanford University.*|.*tanford.*|.*Massachusetts Institute of Technology.*|.*MIT.*|.*Princeton University.*|.*inceton.*|.*Harvard University.*| .*arvard#|.*Yale University.*|.*Yale.*|.*University of Chicago.*|.*Imperial College London.*|.*University of Pennsylvania.*|.*Johns Hopkins University.*|.*University of California Berkeley.*|.*Berkley.*|.*ETH Zurich.*|.*UCL.*|.*Columbia University.*|.*Columbia.*|.*University of California Los Angeles.*|.*University of Toronto.*|.*Cornell University.*|.*Cornell.*|.*Duke University.*|.*University of Michigan-Ann Arbor.*|.*Northwestern University.*|.*Tsinghua University.*|.*Peking University.*|.*National University of Singapore.*", ignore.case = TRUE, x = founders$institution)

top2 = grep(".*University of Washington.*|.*Carnegie Mellon University.*|.*Carnegie.*|.*London School of Economics and Political Science.*|.*LSE.*|.*New York University.*|.*University of Edinburgh.*|.*University of California San Diego.*|.*LMU Munich LMU.*|.*University of Melbourne.*|.*University of British Columbia.*|.*University of Hong Kong.*|.*King’s College London Kings College London.*|.*The University of Tokyo.*|.*École Polytechnique Fédérale de Lausanne.*|.*Lausanne.*|.*Georgia Institute of Technology.*|.*University of Texas at Austin.*|.*Karolinska Institute.*|.*McGill University.*|.*Technical University of Munich.*|.*Heidelberg University.*|.*KU Leuven.*|.*Paris Sciences et Lettres – PSL Research University Paris.*|.*PSL.*|.*The Hong Kong University of Science and Technology.*|.*University of Illinois at Urbana-Champaign.*|.*University of Illinois.*|.*Urbana-Champaign.*|.*Urbana.*|.*Champaign.*|.*Nanyang Technological University .*|.*Australian National University.*", ignore.case = TRUE, x = founders$institution)

top25 = rep(0, nrow(founders))
top50 = rep(0, nrow(founders))
top25[top1] = 1
top50[top2] = 1
founders["top25"] = top25
founders["top50"] = top50
```

```{r eval=FALSE}
founders = aggregate(x = founders[,c("top25", "top50")], by = list(company_id = founders$company_id, person_id = founders$person_id), FUN = max)
peeps = aggregate(x = founders[,c("company_id", "person_id")], by = list(company_id = founders$company_id), FUN = counts)
peeps = peeps[,c(1,2)]
colnames(peeps)[2] = "people"
founders = aggregate(x = founders[,c("top25", "top50")], by = list(company_id = founders$company_id), FUN = sum)
founders = merge(founders, peeps[,c(1,2)], by="company_id")
founders$top25pct = round(100*founders$top25/founders$people)
founders$top50pct = round(100*founders$top50/founders$people)
founders = na.omit(founders[,-c(2,3,4)])

```


### Final Dataset For Analysis

After extracting the individual data aspects (people, company, investments) of the companies, these aspects are joined into a single dataset. The dataset that will be used for this project will depend on the resulting row numbers after different combination of joins.

```{r eval=FALSE}
company_aq = merge(companies, aquisitions, by = "company_id")
#nrow(company_aq)
company_aq_lds = merge(x = company_aq, y = founders, by = "company_id", all.x = TRUE)
#nrow(company_aq_lds)

```

### Preminiary Analysis

To ensure that the dataset is viable for linear regression analysis, we create a preliminary linear model with company aquisition price as the response variable, and several other potential columns in the startups dataset as predictors. We are using the `company_aq` dataset above.

```{r eval=FALSE}
# Data set to be analized, filtering out aquisition values without aquisition price
startups = company_aq_lds[company_aq_lds$aquisition_price_amount>0, ]
nrow(startups)
col_remove = c("entity_type", "entity_id", "parent_id", "name", "permalink", "founded_at", "closed_at", "domain", "homepage_url", "twitter_username", "logo_url", "short_description", "description", "overview", "tag_list", "region", "first_investment_at", "last_investment_at", "first_funding_at", "last_funding_at", "first_milestone_at", "last_milestone_at", "created_by", "created_at", "updated_at")

startups = startups[,-which(colnames(startups) %in% col_remove)]

#Nulls by column
show_nas = function(somedf){
  cols_nas = data.frame(column = colnames(somedf), numNAs = rep(0, ncol(somedf)))
  for(i in 1:ncol(somedf)){
    cols_nas[i, "numNAs"] = length(somedf[is.na(somedf[,i]),i])
  }
  cols_nas$pctNAs = round(100*cols_nas$numNAs / nrow(somedf), digits = 1)
  cols_nas
}
show_nas(startups)

```

We clean `NA` values below:
```{r eval=FALSE}

#clean NAs; introduce "Unknown" for categorical variables where ther are N/As
startups[is.na(startups[,"category_code"]),"category_code"] = "unknown"
startups[is.na(startups[,"country_code"]),"country_code"] = "unknown"
startups[is.na(startups[,"city"]),"city"] = "unknown"
startups[is.na(startups[,"state_code"]),"state_code"] = "unknown"

#For numeric variablestop25pct, top50pct; will convert all of these into a categorical variable that says if people in company attended universities in top 25 universities in worl, then 1. Similarly, if company people (executives), attended universites in the top 26 to 50 universites `istop26_50`, 1; else, 0.
startups[is.na(startups[,"top25pct"]),"top25pct"] = 0
startups[is.na(startups[,"top50pct"]),"top50pct"] = 0
startups$istop25 = ifelse(startups$top25pct > 0, 1, 0)
startups$istop26_50 = ifelse(startups$top50pct > 0, 1, 0)

#Add a few additional predictors
startups$name_length = nchar(startups$normalized_name)

show_nas(startups)
```

With clean data, we can now perform the data split for training and data

```{r eval=FALSE}
#Generate the full data
write_csv(x = startups, path = "./startups.csv")

set.seed(420)
train_index = sample(1:nrow(startups), size = floor(0.8 * nrow(startups)))
startups_tr = startups[train_index,]
startups_te = startups[-train_index,]

#Save the train and test datasest
write_csv(x = startups_tr, path = "./startups_tr.csv")
write_csv(x = startups_te, path = "./startups_te.csv")
```


The dataset to be used, `startups` has `r nrow(startups)` rows of data.

A sample output of the data is below:

```{r echo=FALSE}
#Sample out
knitr::kable(head(startups[, c(
  "aquisition_price_amount",
  "city",
  "funding_total_usd",
  "category_code",
  "funding_rounds",
  "milestones",
  "relationships"
)]))
```

Moreover, a very simple model illustrates feasibility for linear regression analysis:

```{r}
#Linear Model
company_model = lm(log(aquisition_price_amount) ~ log(funding_total_usd+1) + category_code + funding_rounds + milestones + relationships + istop25 + istop26_50, data=startups)
summary(company_model)
```

- $R^2= `r summary(company_model)$r.squared`$
- $\text{p-value} = `r pf(summary(company_model)$fstatistic[1], summary(company_model)$fstatistic[2], summary(company_model)$fstatistic[3], lower.tail=FALSE)`$

In addition, we can see how the linear regression assumptions of normality and constant variance are with the simple model. In the study, will better adjust tthe model in question to more appropriately describe the data with the model including its variance.

```{r message=FALSE, warning=FALSE}
#Plots of the model
par(mfrow = c(1,2))
plot(company_model, which = 1:2)
```





